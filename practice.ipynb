{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a diagram of Hive architecture and explain its main elements.\n",
    "\n",
    "┌────────────────────────┐\n",
    "│     User Interface     │  (CLI, UI, JDBC/ODBC, Thrift)\n",
    "└──────────┬─────────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌────────────────────────┐\n",
    "│     Driver Layer       │\n",
    "│  (Parser, Planner,     │\n",
    "│   Optimizer, Executor) │\n",
    "└──────────┬─────────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌────────────────────────┐\n",
    "│   Metastore (DB)       │\n",
    "│  (Schema, Metadata)    │\n",
    "└──────────┬─────────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌────────────────────────┐\n",
    "│    Execution Engine    │\n",
    "│ (Compiles query into   │\n",
    "│  MapReduce/Tez/Spark)  │\n",
    "└──────────┬─────────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌────────────────────────┐\n",
    "│   Hadoop Distributed   │\n",
    "│   File System (HDFS)   │\n",
    "│   (Actual Data)        │\n",
    "└────────────────────────┘\n",
    "\n",
    "1. User Interface (UI)\n",
    "How users interact with Hive.\n",
    "Tools:\n",
    "    - CLI (Command Line Interface)\n",
    "    - Beeline (JDBC client)\n",
    "    - Hive Web UI\n",
    "    - JDBC/ODBC drivers (for BI tools like Tableau, Power BI)\n",
    "    - Thrift server (remote access)\n",
    "\n",
    "2. Driver Layer\n",
    "Controls the lifecycle of HiveQL statement execution.\n",
    "Components:\n",
    "    - Parser: Parses HiveQL and checks syntax.\n",
    "    - Planner: Generates logical plan.\n",
    "    - Optimizer: Optimizes query (joins, filters, projections).\n",
    "    - Executor: Generates execution plan and coordinates with Execution Engine.\n",
    "\n",
    "3. Metastore\n",
    "- Central repository that stores metadata:\n",
    "    - Table schemas\n",
    "    - Column types\n",
    "    - Table locations\n",
    "    - Partitions\n",
    "    - SerDe (Serialization/Deserialization) information\n",
    "- Typically backed by RDBMS (MySQL, PostgreSQL).\n",
    "- Critical — no query can run without consulting Metastore.\n",
    "\n",
    "4. Execution Engine\n",
    "- Converts optimized logical plan into physical execution:\n",
    "    - MapReduce Jobs\n",
    "    - Apache Tez\n",
    "    - Apache Spark\n",
    "- Controls actual execution of query on Hadoop cluster.\n",
    "- Communicates with Hadoop YARN Resource Manager.\n",
    "\n",
    "5. HDFS (Hadoop Distributed File System)\n",
    "- The actual storage layer for data.\n",
    "- Stores:\n",
    "    - Table data (structured/unstructured)\n",
    "    - Files: ORC, Parquet, Avro, CSV, Text\n",
    "- Hive reads/writes data from HDFS.\n",
    "- You can use external tables pointing to data already in HDFS.\n",
    "Summary Flow: User Query → Parser → Optimizer → Execution Engine → Data fetched from HDFS → Results returned to User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Spark architecture and explain its various components.\n",
    "\n",
    "┌───────────────────────────────────┐\n",
    "│         Spark Application         │\n",
    "└───────────────────────────────────┘\n",
    "                   │\n",
    "                   ▼\n",
    "┌───────────────────────────────────┐\n",
    "│         Driver Program            │\n",
    "│  (SparkContext, Scheduler, etc.)  │\n",
    "└───────────────────────────────────┘\n",
    "                   │\n",
    "                   ▼\n",
    "          ┌─────────────────┐\n",
    "          │  Cluster Manager │  (Standalone / YARN / Mesos / Kubernetes)\n",
    "          └─────────────────┘\n",
    "                   │\n",
    "     ┌─────────────┴─────────────┐\n",
    "     ▼                           ▼\n",
    "┌─────────────┐           ┌─────────────┐\n",
    "│ Executor 1  │           │ Executor N  │\n",
    "│ (Task slot) │    ...    │ (Task slot) │\n",
    "└─────────────┘           └─────────────┘\n",
    "\n",
    "1. Driver Program: \n",
    "The \"main\" program — where SparkContext is created.\n",
    "Coordinates the whole Spark job.\n",
    "Components inside:\n",
    "    SparkContext → entry point of Spark program.\n",
    "    DAG Scheduler → builds stages of tasks.\n",
    "    Task Scheduler → sends tasks to Executors.\n",
    "    Backend → communicates with Cluster Manager.\n",
    "Runs on one machine.\n",
    "\n",
    "2. Cluster Manager: \n",
    "Allocates resources across the cluster.\n",
    "Spark supports:\n",
    "    Standalone cluster manager (built-in)\n",
    "    YARN (Hadoop ecosystem)\n",
    "    Apache Mesos\n",
    "    Kubernetes\n",
    "Manages Executors on worker nodes.\n",
    "\n",
    "3. Executors: \n",
    "Run on worker nodes.\n",
    "Each Executor:\n",
    "    Executes Tasks assigned by Driver.\n",
    "    Has its own memory and caches.\n",
    "    Reports status and results back to Driver.\n",
    "Multiple Executors → parallelism.\n",
    "\n",
    "4. Tasks & Jobs: \n",
    "Spark breaks user action (e.g. collect(), saveAsTextFile()) into Jobs.\n",
    "Jobs → divided into Stages.\n",
    "Stages → divided into Tasks.\n",
    "Tasks → run in parallel on Executors.\n",
    "Tasks are the unit of execution.\n",
    "\n",
    "- Execution Flow\n",
    "    1. User writes Spark code (RDD, DataFrame, etc.)\n",
    "    2. Driver starts → creates SparkContext\n",
    "    3. SparkContext contacts Cluster Manager\n",
    "    4. Executors are launched on Worker Nodes\n",
    "    5. Driver sends Tasks to Executors\n",
    "    6. Executors run Tasks, return results to Driver\n",
    "    7. Final results returned to user\n",
    "\n",
    "Summary\n",
    "Component\tRole\n",
    "Driver Program -\tControls application\n",
    "SparkContext -\tEntry point, manages job\n",
    "Cluster Manager -\tAllocates resources\n",
    "Executors -\tRun tasks in parallel\n",
    "Tasks & Jobs -\tUnits of execution\n",
    "\n",
    "Key Features of Spark Architecture\n",
    "- In-memory computation\n",
    "- Fault tolerance (DAG lineage)\n",
    "- Distributed — scales horizontally\n",
    "- Supports:\n",
    "    - Batch processing\n",
    "    - Streaming\n",
    "    - SQL\n",
    "    - Machine Learning\n",
    "    - Graph processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain partitioning (in hive) with an example.\n",
    "\n",
    "Partitioning in Hive means dividing a large table into smaller parts (partitions) based on the values of a column.\n",
    "    - It improves query performance\n",
    "    - It reduces the amount of data scanned → faster queries\n",
    "\n",
    "How it works:\n",
    "    - You choose one or more columns as partition keys\n",
    "    - Hive stores data in separate directories per partition value\n",
    "    - When you run a query, Hive will only scan relevant partitions\n",
    "\n",
    "Hive Query: Create Partitioned Table\n",
    "CREATE TABLE sales_partitioned (\n",
    "    order_id INT,\n",
    "    product_name STRING,\n",
    "    amount DOUBLE,\n",
    "    country STRING\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List any 2 differences between Coalesce and Repartition (in spark)\n",
    "\n",
    "| Aspect            | `coalesce()`                                                              | `repartition()`                                                                 |\n",
    "| ----------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| 1. **Shuffling** | Tries to **minimize shuffle**                                             | Performs a **full shuffle**                                                     |\n",
    "| 2. **Use Case**  | Mainly used to **reduce** number of partitions (e.g. going from 100 → 10) | Can be used to **increase or decrease** partitions (e.g. 10 → 100, or 100 → 10) |\n",
    "\n",
    "rdd2 = rdd1.coalesce(10),\n",
    "rdd3 = rdd1.repartition(100)\n",
    "\n",
    "Summary:\n",
    "    - coalesce() → used to reduce partitions, minimal shuffle (efficient)\n",
    "    - repartition() → increase/decrease partitions, full shuffle (more flexible, but expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Big Data? Describe the three fundamental aspects that define it.\n",
    "\n",
    "Big Data refers to datasets that are too large, complex, or fast-changing to be processed and analyzed using traditional data-processing tools.\n",
    "\n",
    "In other words — Big Data is not just \"a lot of data,\" it's data that:\n",
    "    - can’t fit in a single machine\n",
    "    - arrives very fast (real-time or near real-time)\n",
    "    - is often messy, unstructured, or of many types\n",
    "\n",
    "The Three Fundamental Aspects of Big Data\n",
    "(often called 3 V’s of Big Data):\n",
    "\n",
    "1. Volume (How Much Data): \n",
    "- Definition: The sheer amount of data generated every second.\n",
    "- Example:\n",
    "    - Facebook generates petabytes of data daily.\n",
    "    - IoT sensors generate massive streams of data.\n",
    "    - Challenge: Storage, distributed processing.\n",
    "\n",
    "2. Velocity (How Fast Data Arrives)\n",
    "- Definition: The speed at which new data is generated and moves around.\n",
    "- Example:\n",
    "    - Stock market data → milliseconds latency.\n",
    "    - Real-time fraud detection → fast decision making needed.\n",
    "    - Challenge: Real-time processing, streaming analytics.\n",
    "\n",
    "3. Variety (How Many Types of Data)\n",
    "- Definition: Different types of data — structured, semi-structured, unstructured.\n",
    "- Example:\n",
    "    - Structured → Tables in databases.\n",
    "    - Semi-structured → JSON, XML.\n",
    "    - Unstructured → Text, video, images, audio, social media posts.\n",
    "    - Challenge: Data integration, processing diverse formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the key differences between a Data Lake and a Data Warehouse? \n",
    "\n",
    "Aspect\tData Lake\tData Warehouse\n",
    "Data Type\tRaw data — structured, semi-structured, unstructured\tMostly structured data (organized into tables, schema-on-write)\n",
    "Schema\tSchema-on-read → apply schema when you read the data\tSchema-on-write → schema defined before data is written\n",
    "Purpose\tData exploration, advanced analytics, machine learning\tBusiness reporting, BI dashboards, historical analysis\n",
    "Data Storage\tLow-cost storage (HDFS, S3, Blob Storage)\tHigh-performance storage (specialized databases like Snowflake, Redshift, BigQuery)\n",
    "Users\tData scientists, data engineers\tBusiness analysts, BI professionals\n",
    "Data Processing\tBatch + Real-time (with Spark, Flink, etc.)\tMostly batch (SQL-based)\n",
    "Cost\tCheaper (store all data, no need to clean before storage)\tMore expensive (optimized, clean, curated data)\n",
    "Flexibility\tVery flexible — any type of data\tLess flexible — structured data only\n",
    "Examples\tHadoop, Amazon S3 Data Lake, Azure Data Lake\tSnowflake, Google BigQuery, Amazon Redshift, Teradata\n",
    "\n",
    "- Summary:\n",
    "    - Data Lake = store everything → flexible → good for AI/ML → cheaper\n",
    "    - Data Warehouse = store cleaned, structured data → good for business reporting → faster queries\n",
    "\n",
    "- When to use:\n",
    "Use Case\tRecommended\n",
    "Store all raw data from many sources\tData Lake\n",
    "Build BI reports for CEO & management\tData Warehouse\n",
    "Train machine learning models on image/text/audio data\tData Lake\n",
    "Run complex SQL queries with guaranteed fast performance\tData Warehouse\n",
    "\n",
    "- Conclusion:\n",
    "    - Data Lakes are flexible, cheap, great for ML\n",
    "    - Data Warehouses are optimized for fast BI reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the different types of NoSQL databases? Write a brief note.\n",
    "\n",
    "Types of NoSQL Databases — A Quick Guide\n",
    "NoSQL = \"Not Only SQL\" → designed for:\n",
    "- High scalability\n",
    "- Handling unstructured or semi-structured data\n",
    "- Flexibility → schema-less\n",
    "\n",
    "There are 4 main types of NoSQL databases:\n",
    "\n",
    "1. Key-Value Stores\n",
    "Data stored as key-value pairs (like a dictionary or hashmap)\n",
    "Best for:\n",
    "Simple lookups\n",
    "Session management\n",
    "Caching\n",
    "User profile storage\n",
    "Examples:\n",
    "Redis\n",
    "Amazon DynamoDB\n",
    "Memcached\n",
    "\n",
    "2. Document Stores\n",
    "Data stored as documents (typically JSON, BSON, or XML)\n",
    "Documents can have nested structures and are schema-less\n",
    "Best for:\n",
    "Content management\n",
    "Product catalogs\n",
    "Mobile apps\n",
    "Event logging\n",
    "Examples:\n",
    "MongoDB\n",
    "CouchDB\n",
    "Amazon DocumentDB\n",
    "\n",
    "3. Column-Family Stores\n",
    "Data stored in columns, not rows\n",
    "Very good for highly scalable, distributed systems\n",
    "Best for:\n",
    "Time-series data\n",
    "Analytical queries\n",
    "Event logging\n",
    "Large datasets across multiple machines\n",
    "Examples:\n",
    "Apache Cassandra\n",
    "Apache HBase\n",
    "ScyllaDB\n",
    "\n",
    "4. Graph Databases\n",
    "Data stored as nodes and edges\n",
    "Best for modeling relationships (networks, social graphs, hierarchies)\n",
    "Best for:\n",
    "Social networks\n",
    "Recommendation engines\n",
    "Fraud detection\n",
    "Knowledge graphs\n",
    "Examples:\n",
    "Neo4j\n",
    "Amazon Neptune\n",
    "ArangoDB\n",
    "TigerGraph\n",
    "\n",
    "- Summary Table:\n",
    "Type\tData Model\tExample Use Case\tExample DBs\n",
    "Key-Value Store\tKey → Value\tCaching, session store\tRedis, DynamoDB\n",
    "Document Store\tDocument (JSON/BSON)\tContent mgmt, catalogs\tMongoDB, CouchDB\n",
    "Column-Family\tColumn families\tTime-series, analytics\tCassandra, HBase\n",
    "Graph Database\tNodes + Edges\tSocial networks, fraud\tNeo4j, Neptune\n",
    "\n",
    "- Final Takeaway:\n",
    "    - NoSQL = flexible, scalable, handles Big Data and unstructured data\n",
    "    - Choice depends on use case:\n",
    "Simple key lookups → Key-Value\n",
    "Flexible docs → Document DB\n",
    "Fast analytical queries → Column-Family\n",
    "Complex relationships → Graph DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main features of Apache Kafka that make it a popular choice for real-time data\n",
    "streaming?\n",
    "\n",
    "1. High Throughput\n",
    "    - Kafka can process millions of messages per second\n",
    "    - Handles large data volumes easily\n",
    "    - Suitable for both batch and real-time streaming\n",
    "\n",
    "2. Scalability\n",
    "    - Kafka is horizontally scalable → just add more brokers (servers)\n",
    "    - Supports partitioning of topics → enables parallelism\n",
    "\n",
    "3. Fault Tolerance\n",
    "    - Kafka is a distributed system → data is replicated across multiple brokers\n",
    "    - If one broker fails → others continue\n",
    "    - Ensures high availability and durability\n",
    "\n",
    "4. Durability\n",
    "    - Kafka persists messages to disk\n",
    "    - Messages can be retained for a configurable retention period\n",
    "    - Supports long-term replay of data\n",
    "\n",
    "5. Real-time Stream Processing\n",
    "    - Kafka integrates with stream processing frameworks:\n",
    "        - Apache Spark Streaming\n",
    "        - Apache Flink\n",
    "        - Kafka Streams API (built-in lightweight stream processing)\n",
    "    - Enables real-time analytics\n",
    "\n",
    "6. Publish-Subscribe Model\n",
    "    - Kafka is a Pub-Sub system:\n",
    "        - Producers publish data\n",
    "        - Consumers subscribe and consume data\n",
    "    - Supports:\n",
    "        - One-to-many delivery\n",
    "        - Multiple consumers can read same stream independently\n",
    "\n",
    "Feature\tBenefit\n",
    "High Throughput - Process millions of messages/sec\n",
    "Scalability -\tAdd brokers and partitions easily\n",
    "Fault Tolerance -\tData replicated → high availability\n",
    "Durability -\tMessages persisted to disk\n",
    "Real-time Processing -\tInstant streaming and analytics\n",
    "Pub-Sub Model -\tEasy decoupling of producers/consumers\n",
    "Exactly Once -\tReliable, accurate processing\n",
    "Large Ecosystem -\tEasy to integrate with other tools\n",
    "\n",
    "- Conclusion\n",
    "    - Apache Kafka is fast, scalable, reliable, fault-tolerant → that’s why it’s widely used for:\n",
    "        - Real-time analytics\n",
    "        - Event-driven architectures\n",
    "        - Data pipelines\n",
    "        - Streaming ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDFS Commands\n",
    "\n",
    "Print directory with details: \n",
    "hdfs dfs -ls -h InputDir\n",
    "\n",
    "Upload multiple files: \n",
    "hdfs dfs -put *.txt XYZ\n",
    "\n",
    "Display file line by line: \n",
    "hdfs dfs -cat file.txt | less\n",
    "\n",
    "Recursive delete: \n",
    "hdfs dfs -rm -r SampleDir\n",
    "\n",
    "Copy from HDFS to local: \n",
    "hdfs dfs -get testfile ./\n",
    "\n",
    "Copy file1.txt to OutputDir as file2.txt: \n",
    "hdfs dfs -cp InputDir/file1.txt OutputDir/file2.txt\n",
    "\n",
    "Delete empty directory XYZ: \n",
    "hdfs dfs -rmdir XYZ\n",
    "\n",
    "Recursively list files under SampleDir: \n",
    "hdfs dfs -ls -R SampleDir\n",
    "\n",
    "Change permission of file.txt to read-only (444): \n",
    "hdfs dfs -chmod 444 file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apache Spark RDD Operations\n",
    "\n",
    "Create RDD: \n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "Print first 4 elements: \n",
    "print(rdd.take(4))\n",
    "\n",
    "Print first element: \n",
    "print(rdd.first())\n",
    "\n",
    "Load text file into RDD: \n",
    "file_rdd = sc.textFile(\"/path/to/file.txt\")\n",
    "\n",
    "Number of elements: \n",
    "num_elements = rdd.count()\n",
    "print(f\"Number of elements in RDD: {num_elements}\")\n",
    "\n",
    "Sum of elements: \n",
    "sum_elements = rdd.reduce(lambda x, y: x + y)\n",
    "print(f\"Sum of all elements in RDD: {sum_elements}\")\n",
    "\n",
    "map → applies a function to each element of the RDD\n",
    "Returns a new RDD with transformed elements\n",
    "\n",
    "Example: Multiply each element by 2: \n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "print(mapped_rdd.collect())\n",
    "Explanation:\n",
    "\n",
    "filter → keeps only elements that satisfy a condition\n",
    "Returns a new RDD with filtered elements\n",
    "\n",
    "Example: Keep only even numbers: \n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(filtered_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hive Queries\n",
    "\n",
    "Create database: \n",
    "CREATE DATABASE emp;\n",
    "\n",
    "Load data from local file: \n",
    "LOAD DATA LOCAL INPATH '/path/to/your/file.csv'\n",
    "INTO TABLE orders1;\n",
    "\n",
    "Create external table: \n",
    "USE emp;\n",
    "CREATE EXTERNAL TABLE employee (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    location STRING,\n",
    "    dep STRING,\n",
    "    designation STRING,\n",
    "    salary DOUBLE\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/hive/warehouse/emp/employee';\n",
    "\n",
    "Inner join on Table1 and Table2: \n",
    "SELECT t1.*, t2.*\n",
    "FROM Table1 t1\n",
    "INNER JOIN Table2 t2\n",
    "ON t1.id = t2.id;\n",
    "\n",
    "average salary of employees based on location: \n",
    "SELECT location, AVG(salary) AS avg_salary FROM employee GROUP BY location;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mongoDB\n",
    "\n",
    "Create collection: \n",
    "db.createCollection(\"product_collection\")\n",
    "\n",
    "Insert documents: \n",
    "db.product_collection.insertMany([\n",
    "    { name: \"AC\", rating: 5, brand: \"Samsung\" },\n",
    "    { name: \"Laptop\", rating: 4, brand: \"Dell\" },\n",
    "    { name: \"Mobile\", rating: 5, brand: \"Apple\" },\n",
    "    { name: \"TV\", rating: 3, brand: \"Sony\" },\n",
    "    { name: \"Refrigerator\", rating: 5, brand: \"LG\" }\n",
    "])\n",
    "\n",
    "Find products with 5/5 rating: \n",
    "db.product_collection.find({ rating: 5 })\n",
    "\n",
    "Update name 'AC' → 'Air conditioner': \n",
    "db.product_collection.updateMany(\n",
    "    { name: \"AC\" },\n",
    "    { $set: { name: \"Air conditioner\" } }\n",
    ")\n",
    "\n",
    "Print updated record: \n",
    "db.product_collection.find({ name: \"Air conditioner\" })\n",
    "\n",
    "Fetch users with age >= 34: \n",
    "db.users.find({ age: { $gte: 34 } })\n",
    "\n",
    "Delete user Ravi: \n",
    "db.users.deleteOne({ name: \"Ravi\" })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset: \n",
    "df = spark.read.csv(\"/path/to/qs_world_university_rankings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "Count distinct Institutions: \n",
    "num_institutions = df.select(\"Institution\").distinct().count()\n",
    "print(f\"Number of Institutions: {num_institutions}\")\n",
    "\n",
    "Institutions from India: \n",
    "num_india_institutions = df.filter(df[\"Location Full\"] == \"India\").select(\"Institution\").distinct().count()\n",
    "print(f\"Number of Institutions from India: {num_india_institutions}\")\n",
    "\n",
    "Average Citations per Faculty for India: \n",
    "df.filter(df[\"Location Full\"] == \"India\").selectExpr(\"avg(`Citations per Faculty`) as avg_citations\").show()\n",
    "\n",
    "Institutions with 100% International Students: \n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "df = df.withColumn(\"International Students\", regexp_replace(col(\"International Students\"), \"%\", \"\").cast(\"double\"))\n",
    "df.filter(col(\"International Students\") == 100).select(\"Institution\", \"Location Full\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f307dc8-e95c-476e-b558-ef1c40d6893d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KWl87C1X3Rey"
   },
   "source": [
    "#### 1. What’s the overall minimum, maximum and average salary from the dataset? ( 6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIZAJg5j7CzX"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SalaryAnalysis\").getOrCreate()\n",
    "file_path = \"path/to/mba_placement.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "columns = df.columns\n",
    "print(\"Columns:\", columns)\n",
    "possible_salary_columns = [col for col in columns if 'salary' in col.lower()]\n",
    "print(\"Possible salary columns:\", possible_salary_columns)\n",
    "if possible_salary_columns:\n",
    "    salary_col = possible_salary_columns[0]\n",
    "    stats = df.select(\n",
    "        min(salary_col).alias(\"min_salary\"),\n",
    "        max(salary_col).alias(\"max_salary\"),\n",
    "        avg(salary_col).alias(\"avg_salary\")\n",
    "    )\n",
    "    stats.show()\n",
    "else:\n",
    "    print(f\"No salary column found. Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a02b93a3-b33d-411c-bb6d-6693fbe303b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LZrfZlwN3Re0"
   },
   "source": [
    "#### 2. How many  female candidate are not placed ?  ( 4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiS3CBea3Re0"
   },
   "outputs": [],
   "source": [
    "female_not_placed_count = df.filter(\n",
    "    (col(\"gender\") == \"F\") & \n",
    "    (col(\"status\") == \"Not Placed\")\n",
    ").count()\n",
    "print(f\"Number of female candidates not placed: {female_not_placed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fccd4a1-260e-47db-b630-8bbe55acac9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "09V5Lb6I3Re1"
   },
   "source": [
    "#### `3`. Out of total male candidates placed, how many does not have any work experience ? (3marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGOqXT193Re2"
   },
   "outputs": [],
   "source": [
    "male_placed_df = df.filter(\n",
    "    (lower(trim(col(\"gender\"))) == \"m\") &\n",
    "    (lower(trim(col(\"status\"))) == \"placed\")\n",
    ")\n",
    "male_placed_no_workex_count = male_placed_df.filter(\n",
    "    (lower(trim(col(\"workex\"))) == \"no\")\n",
    ").count()\n",
    "total_male_placed = male_placed_df.count()\n",
    "print(f\"Total male candidates placed: {total_male_placed}\")\n",
    "print(f\"Number of male candidates placed with NO work experience: {male_placed_no_workex_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e611b4c8-1705-4ba2-a538-212f2e7cc60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4t4oncKi3Re3"
   },
   "source": [
    "#### 4. Remove the feature 'sl_no' and also remove null values from the DataFrame. (2 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMcHDNIp3Re4"
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.drop('sl_no')\n",
    "df_cleaned = df_cleaned.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc4b51f-c180-4517-9e9c-443c73d1d1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2fyV9iJR3Re4"
   },
   "source": [
    "### 1.\tConvert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztT_JhDi3Re5"
   },
   "outputs": [],
   "source": [
    "string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "print(f\"String columns to index: {string_cols}\")\n",
    "from pyspark.ml import Pipeline\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid='keep')\n",
    "    for col in string_cols\n",
    "]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "df_final = df_indexed.drop(*string_cols)\n",
    "df_final.show(5)\n",
    "final_string_cols = [field.name for field in df_final.schema.fields if isinstance(field.dataType, StringType)]\n",
    "print(f\"Remaining string columns: {final_string_cols}\")\n",
    "final_row_count = df_final.count()\n",
    "print(f\"Total rows in final DataFrame: {final_row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3038f4a1-df50-415b-8d45-d1f0a648a685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "C3tYKaY73Re5"
   },
   "source": [
    "### 2. Using vectorAssembler combines all columns (except target column i.e., 'salary') of spark DataFrame into single column (name as features). Make sure DataFrame now contains only two columns features and salary. ( 5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMGuPLfF3Re5"
   },
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df_numeric.columns if col != 'salary']\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_final = assembler.transform(df_numeric)\n",
    "df_final = df_final.select(\"features\", \"salary\")\n",
    "print(\"Final DataFrame with features and salary:\")\n",
    "df_final.show(5, truncate=False)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374edecf-702b-477f-a2ca-b5ac553913e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VzprItcI3Re6"
   },
   "source": [
    "### 3 .\tSplit the vectorized dataframe into training and test sets with one fourth records being held for testing (**3** marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAaMzDfO3Re6"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df_final.randomSplit([0.75, 0.25], seed=42)\n",
    "print(f\"Training set count: {train_df.count()}\")\n",
    "print(f\"Test set count: {test_df.count()}\")\n",
    "print(\"Training Set Sample:\")\n",
    "train_df.show(5, truncate=False)\n",
    "print(\"Test Set Sample:\")\n",
    "test_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5373a2-ee31-44b4-afb1-98a411d50d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3_rA9BIH3Re6"
   },
   "source": [
    "### 4.\tBuild a LinearRegression model on train set  use featuresCol=\"features\" and  'salary'(6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28Hq4UBW3Re6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "lr_model = lr.fit(train_df)\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "test_results = lr_model.evaluate(test_df)\n",
    "print(f\"RMSE (Root Mean Squared Error): {test_results.rootMeanSquaredError}\")\n",
    "print(f\"MAE (Mean Absolute Error): {test_results.meanAbsoluteError}\")\n",
    "print(f\"R2 (R-squared): {test_results.r2}\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5859d786-d9d9-4201-baf6-e1ff60cae45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Bnphd20Q3Re6"
   },
   "source": [
    "### 5. Perform prediction on the testing data and Print MSE value? (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t092vhvL3Re7"
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"features\", \"salary\", \"prediction\").show(5, truncate=False)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"salary\", predictionCol=\"prediction\", metricName=\"mse\"\n",
    ")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "predictions.select(\"features\", \"salary\", \"prediction\").show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BDA_ESA_August 2025_Solution",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
